import time
from google import genai

# For extracting vertex experiment details.
from google.cloud import aiplatform
from google.cloud.aiplatform.metadata import context
from google.cloud.aiplatform.metadata import utils as metadata_utils

# For data handling.
import jsonlines
import pandas as pd

# For visualization.
import plotly.graph_objects as go
from google.oauth2.service_account import Credentials
from plotly.subplots import make_subplots

# For evaluation metric computation.
from rouge_score import rouge_scorer
from tqdm import tqdm
from google.genai import types

# For fine tuning Gemini model.
import vertexai
from vertexai.generative_models import GenerativeModel

google_sa_json = {
        ...
    }

required_scopes = ["https://www.googleapis.com/auth/cloud-platform"]
credentials = Credentials.from_service_account_info(google_sa_json, scopes=required_scopes)

project_id = "wonder-ai1"
region = "us-central1"
vertexai.init(project=project_id, location=region, credentials=credentials)
client = genai.Client(vertexai=True, credentials=credentials, project=project_id, location=region)
base_model = "gemini-1.5-pro-002"


testing_data_path = "finetune_data_test.csv"
test_data = pd.read_csv(testing_data_path, delimiter="\t")

config = {
        "temperature": 0.1,
        "max_output_tokens": 8096,
    }

def test_gemini_model():
    test_doc = test_data.loc[0, "input_text"]

    prompt = f"""
    {test_doc}
    """
    response = client.models.generate_content(contents=prompt, config=config, model=base_model)
    text = response.text
    print(text)


def run_evaluation(model, corpus: list[dict]) -> pd.DataFrame:
    """Runs evaluation for the given model and data.

    Args:
      model: The generation model.
      corpus: The test data.

    Returns:
      A pandas DataFrame containing the evaluation results.
    """
    scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
    records = []
    for item in tqdm(corpus):
        document = item.get("input_text")
        summary = item.get("output_text")

        config = {
            "temperature": 0.1,
            "max_output_tokens": 8096,
        }

        # Catch any exception that occur during model evaluation.
        try:
            response = client.models.generate_content(contents=document, config=config, model=model)
            # Check if response is generated by the model, if response is empty then continue to next item.
            if not (
                response
                and response.candidates
                and response.candidates[0].content.parts
            ):
                print(
                    f"\nModel has blocked the response for the document.\n Response: {response}\n Document: {document}"
                )
                continue

            # Calculates the ROUGE score for a given reference and generated summary.
            scores = scorer.score(target=summary, prediction=response.text)

            # Append the results to the records list
            records.append(
                {
                    "document": document,
                    "summary": summary,
                    "generated_summary": response.text,
                    "scores": scores,
                    "rouge1_precision": scores.get("rouge1").precision,
                    "rouge1_recall": scores.get("rouge1").recall,
                    "rouge1_fmeasure": scores.get("rouge1").fmeasure,
                    "rouge2_precision": scores.get("rouge2").precision,
                    "rouge2_recall": scores.get("rouge2").recall,
                    "rouge2_fmeasure": scores.get("rouge2").fmeasure,
                    "rougeL_precision": scores.get("rougeL").precision,
                    "rougeL_recall": scores.get("rougeL").recall,
                    "rougeL_fmeasure": scores.get("rougeL").fmeasure,
                }
            )
        except AttributeError as attr_err:
            print("Attribute Error:", attr_err)
            continue
        except Exception as err:
            print("Error:", err)
            continue
    return pd.DataFrame(records)


def evaluation_base_model():
    corpus = test_data.to_dict(orient="records")
    evaluation_df = run_evaluation(base_model, corpus)
    evaluation_df_stats = evaluation_df.dropna().describe()
    print(evaluation_df_stats)
    print("Mean rougeL_precision is", evaluation_df_stats.rougeL_precision["mean"])


def fine_tune_model():
    tuned_model_display_name = "gemini-1.5-pro-fine-tune-0001"  # @param {type:"string"}

    training_dataset = {
        "gcs_uri": f"gs://cvinfo/training/gemini/finetune_data_train1.jsonl",
    }

    validation_dataset = types.TuningValidationDataset(
        gcs_uri=f"gs://cvinfo/training/gemini/finetune_data_validation1.jsonl"
    )

    print("=" * 60)
    print("开始创建并启动 Gemini 微调作业...")
    print(f"  - 基础模型: {base_model}")
    print(f"  - 训练数据 GCS 路径: {training_dataset['gcs_uri']}")
    print(f"  - 验证数据 GCS 路径: {validation_dataset.gcs_uri}")
    print(f"  - 输出模型名称: {tuned_model_display_name}")
    print("=" * 60)

    sft_tuning_job = client.tunings.tune(
        base_model=base_model,
        training_dataset=training_dataset,
        config=types.CreateTuningJobConfig(
            tuned_model_display_name=tuned_model_display_name,
            validation_dataset=validation_dataset,
        ),
    )

    tuning_job = client.tunings.get(name=sft_tuning_job.name)
    print(f"微调作业已提交，作业名称: {tuning_job.name}")
    print("正在等待作业完成，这可能需要一个多小时...")
    # Wait for job completion

    running_states = [
        "JOB_STATE_PENDING",
        "JOB_STATE_RUNNING",
    ]

    while tuning_job.state.name in running_states:
        print(".", end="")
        tuning_job = client.tunings.get(name=tuning_job.name)
        time.sleep(20)
    print("\n作业已结束运行。")

    # --- 4. 检查最终状态并处理结果 (核心修改部分) ---
    final_state = tuning_job.state
    print(f"作业最终状态: {final_state.name}")

    if final_state == types.JobState.JOB_STATE_SUCCEEDED:
        # 只有在成功时，tuned_model才会有值
        tuned_model_info = tuning_job.tuned_model
        print("\n微调成功！")
        print(f"  -> 新模型资源名称: {tuned_model_info.name}")
        print(f"  -> 新模型显示名称: {tuned_model_info.display_name}")
        print(f"  -> 基础模型: {tuned_model_info.base_model}")
        # endpoint属性可能不存在，直接打印模型名称是更安全的做法
        # 您需要手动将其部署到端点

    elif final_state == types.JobState.JOB_STATE_FAILED:
        # 如果失败，打印错误信息
        job_error = tuning_job.error
        print("\n微调作业失败！")
        print(f"  -> 错误代码: {job_error.code}")
        print(f"  -> 错误信息: {job_error.message}")
        # 详细错误信息可能在 'details' 字段中
        if job_error.details:
            print("  -> 错误详情:")
            for detail in job_error.details:
                print(f"     - {detail}")
    else:
        # 处理其他状态，如取消
        print(f"\n作业以状态 {final_state.name} 结束，非成功状态。")


def get_loss():
    tune_jon_name = f"projects/{project_id}/locations/us-central1/tuningJobs/5588068647588331520"
    tuning_job = client.tunings.get(name=tune_jon_name)
    tuned_model = tuning_job.tuned_model.endpoint
    experiment_name = tuning_job.experiment

    print("Tuned model experiment", experiment_name)
    print("Tuned model endpoint resource name:", tuned_model)

    experiment = aiplatform.Experiment(experiment_name=experiment_name)
    filter_str = metadata_utils._make_filter_string(
        schema_title="system.ExperimentRun",
        parent_contexts=[experiment.resource_name],
    )
    experiment_run = context.Context.list(filter_str)[0]
    tensorboard_run_name = f"{experiment.get_backing_tensorboard_resource().resource_name}/experiments/{experiment.name}/runs/{experiment_run.name.replace(experiment.name, '')[1:]}"
    tensorboard_run = aiplatform.TensorboardRun(tensorboard_run_name)
    metrics = tensorboard_run.read_time_series_data()

    def get_metrics(metric: str = "/train_total_loss"):
        """
        Get metrics from Tensorboard.

        Args:
          metric: metric name, eg. /train_total_loss or /eval_total_loss.
        Returns:
          steps: list of steps.
          steps_loss: list of loss values.
        """
        loss_values = metrics[metric].values
        steps_loss = []
        steps = []
        for loss in loss_values:
            steps_loss.append(loss.scalar.value)
            steps.append(loss.step)
        return steps, steps_loss

    train_loss = get_metrics(metric="/train_total_loss")
    eval_loss = get_metrics(metric="/eval_total_loss")

    def plot_metrics():
        fig = make_subplots(
            rows=1, cols=2, shared_xaxes=True, subplot_titles=("Train Loss", "Eval Loss")
        )

        # Add traces
        fig.add_trace(
            go.Scatter(x=train_loss[0], y=train_loss[1], name="Train Loss", mode="lines"),
            row=1,
            col=1,
        )
        fig.add_trace(
            go.Scatter(x=eval_loss[0], y=eval_loss[1], name="Eval Loss", mode="lines"),
            row=1,
            col=2,
        )

        # Add figure title
        fig.update_layout(title="Train and Eval Loss", xaxis_title="Steps", yaxis_title="Loss")

        # Set x-axis title
        fig.update_xaxes(title_text="Steps")

        # Set y-axes titles
        fig.update_yaxes(title_text="Loss")

        # Show plot
        fig.show()

    # plot_metrics()
    print(train_loss)
    print(eval_loss)


def load_fine_tune_model():
    tune_jon_name = "projects/763866318278/locations/us-central1/tuningJobs/5588068647588331520"
    tuning_job = client.tunings.get(name=tune_jon_name)
    tuned_model = tuning_job.tuned_model.endpoint
    prompt = f"Hello, Who are you?"
    if True:
        # Test with the loaded model.
        print("***Testing***")
        print(
            client.models.generate_content(
                model=tuned_model, contents=prompt, config=config
            ).text
        )
    else:
        print("State:", tuning_job.state.name.state)
        print("Error:", tuning_job.state.name.error)


def evaluation_post_model_tuning():
    tune_jon_name = "projects/763866318278/locations/us-central1/tuningJobs/5588068647588331520"
    tuning_job = client.tunings.get(name=tune_jon_name)
    tuned_model = tuning_job.tuned_model.endpoint
    corpus = test_data.to_dict(orient="records")
    # run evaluation
    evaluation_df_post_tuning = run_evaluation(tuned_model, corpus)
    evaluation_df_post_tuning_stats = evaluation_df_post_tuning.dropna().describe()
    print(
        "Mean rougeL_precision is", evaluation_df_post_tuning_stats.rougeL_precision["mean"]
    )

    evaluation_df = run_evaluation(base_model, corpus)
    evaluation_df_stats = evaluation_df.dropna().describe()
    improvement = round(
        (
                (
                        evaluation_df_post_tuning_stats.rougeL_precision["mean"]
                        - evaluation_df_stats.rougeL_precision["mean"]
                )
                / evaluation_df_stats.rougeL_precision["mean"]
        )
        * 100,
        2,
    )
    print(
        f"Model tuning has improved the rougeL_precision by {improvement}% (result might differ based on each tuning iteration)"
    )


if __name__ == '__main__':
    tune_jon_name = "projects/763866318278/locations/us-central1/tuningJobs/5588068647588331520"
    tuning_job = client.tunings.get(name=tune_jon_name)
    tuned_model = tuning_job.tuned_model.endpoint
    print(tuned_model)
